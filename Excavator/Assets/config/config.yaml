# default:
#     trainer: ppo
#     batch_size: 1024
#     beta: 5.0e-3
#     buffer_size: 10240
#     epsilon: 0.2
#     hidden_units: 128
#     lambd: 0.95
#     learning_rate: 3.0e-4
#     learning_rate_schedule: linear
#     max_steps: 5.0e5
#     memory_size: 128
#     normalize: false
#     num_epoch: 3
#     num_layers: 2
#     time_horizon: 64
#     sequence_length: 64
#     summary_freq: 10000
#     use_recurrent: false
#     vis_encode_type: simple
#     reward_signals:
#         extrinsic:
#             strength: 1.0
#             gamma: 0.99


# Excavator0:
#     max_steps: 2.0e6
#     batch_size: 128
#     buffer_size: 2048
#     beta: 1.0e-2
#     hidden_units: 256
#     summary_freq: 60000
#     time_horizon: 64
#     num_layers: 2
behaviors:
  # BehaviorPPO:
  Excavator0:
    trainer_type: ppo

    hyperparameters:
      # Hyperparameters common to PPO and SAC
      batch_size: 1024
      buffer_size: 10240
      learning_rate: 3.0e-4
      learning_rate_schedule: linear

      # PPO-specific hyperparameters
      # Replaces the "PPO-specific hyperparameters" section above
      beta: 5.0e-3
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3

    # Configuration of the neural network (common to PPO/SAC)
    network_settings:
      vis_encode_type: simple
      normalize: false
      hidden_units: 128
      num_layers: 2
      # memory
      memory:
        sequence_length: 64
        memory_size: 256

    # Trainer configurations common to all trainers
    max_steps: 5.0e5
    time_horizon: 64
    summary_freq: 1000
    keep_checkpoints: 5
    checkpoint_interval: 5000
    threaded: false
    init_path: null

    # # behavior cloning
    # behavioral_cloning:
    #   demo_path: Project/Assets/ML-Agents/Examples/Pyramids/Demos/ExpertPyramid.demo
    #   strength: 0.5
    #   steps: 150000
    #   batch_size: 512
    #   num_epoch: 3
    #   samples_per_update: 0

    # reward_signals:
    #   # environment reward (default)
    #   extrinsic:
    #     strength: 1.0
    #     gamma: 0.99

    #   # curiosity module
    #   curiosity:
    #     strength: 0.02
    #     gamma: 0.99
    #     encoding_size: 256
    #     learning_rate: 3.0e-4

    #   # GAIL
    #   gail:
    #     strength: 0.01
    #     gamma: 0.99
    #     encoding_size: 128
    #     demo_path: Project/Assets/ML-Agents/Examples/Pyramids/Demos/ExpertPyramid.demo
    #     learning_rate: 3.0e-4
    #     use_actions: false
    #     use_vail: false

    # # self-play
    # self_play:
    #   window: 10
    #   play_against_latest_model_ratio: 0.5
    #   save_steps: 50000
    #   swap_steps: 2000
    #   team_change: 100000